<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Hairmony</title>
    <link rel="shortcut icon" type="image/jpg" href="img/favicon.ico" />
    <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css"> -->
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>

</head>

<body>
    <nav class="navbar is-dark" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                    <img src="img/Microsoft-logo.svg" alt="Mixed Reality & AI Lab â€“ Cambridge" style="height: 1.4rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-start">
                    <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                        Mixed Reality & AI
                    </a>
                </div>
                <div class="navbar-end">
                    <a class="navbar-item" href="https://asia.siggraph.org/2024/">
                        <img class="is-hidden-touch" src="img/sa-logo-white.png" alt="SIGGRAPH Asia 2024">
                        <img class="is-hidden-desktop" src="img/sa-logo-black.png" alt="SIGGRAPH Asia 2024">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-2 has-text-centered">Hairmony</h1>
            <p class="subtitle is-4 has-text-centered">Fairness-aware hairstyle classification</p>
            <p class="subtitle is-5 has-text-centered has-text-grey">SIGGRAPH Asia 2024</p>
            <p class="subtitle is-6 has-text-centered authors" style="line-height: 1.5;">
                <span><a href="https://gmeishvili.github.io/" class="has-tooltip-bottom" data-tooltip="* denotes equal contribution">Givi&nbsp;Meishvili</a><sup>*</sup></span>
                <span><a href="mailto:jaclemoe@microsoft.com" class="has-tooltip-bottom" data-tooltip="* denotes equal contribution">James&nbsp;Clemoes</a><sup>*</sup></span>
                <span><a href="https://chewitt.me/" class="has-tooltip-bottom" data-tooltip="* denotes equal contribution">Charlie&nbsp;Hewitt</a><sup>*</sup></span>
                <span><a href="mailto:zhosenie@microsoft.com">Zafiirah&nbsp;Hosenie</a></span>
                <span><a href="mailto:xianxiao@microsoft.com">Xiao-Xian</a></span>
                <span><a href="mailto:madelago@microsoft.com">Martin&nbsp;de&nbsp;La&nbsp;Gorce</a></span>
                <span><a href="mailto:Tibor.Takacs@microsoft.com">Tibor&nbsp;Takacs</a></span>
                <span><a href="mailto:tabaltru@microsoft.com">Tadas&nbsp;Baltru&scaron;aitis</a></span>
                <span><a href="mailto:acriminisi@microsoft.com">Antonio&nbsp;Criminisi</a></span>
                <span><a href="mailto:chynamcrae@microsoft.com">Chyna&nbsp;McRae</a></span>
                <span><a href="mailto:ngj2@psu.edu">Nina&nbsp;Jablonski</a></span>
                <span><a href="mailto:mawilczk@microsoft.com">Marta&nbsp;Wilczkowiak</a></span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <a href="TODO" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
            </a>
            <a href="TODO" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <a href="TODO" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fab fa-youtube" aria-hidden="true"></i></span>
                <span>Video</span>
            </a>
            <a href="https://github.com/microsoft/hairmony" class="button is-rounded is-link is-light">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Dataset</span>
            </a>
        </div>
    </section>
    <section>
        <div class="container is-max-desktop">
            <figure class="image is-16by9">
                <iframe class="has-ratio" width="640" height="360" src="TODO" frameborder="0" allowfullscreen></iframe>
            </figure>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                We present a method for prediction of a person's hairstyle from a single image.
                Despite growing use cases in user digitization and enrollment for virtual experiences, available methods are limited, particularly in the range of hairstyles they can capture.
                Human hair is extremely diverse and lacks any universally accepted description or categorization, making this a challenging task.
                Most current methods rely on parametric models of hair at a strand level.
                These approaches, while very promising, are not yet able to represent short, frizzy, coily hair and gathered hairstyles.
                We instead choose a classification approach which <i>can</i> represent the diversity of hairstyles required for a truly robust and inclusive system.
                Previous classification approaches have been restricted by poorly labeled data that lacks diversity, imposing constraints on the usefulness of any resulting enrollment system.
                We use only synthetic data to train our models. This allows for explicit control of diversity of hairstyle attributes, hair colors, facial appearance, poses, environments and other parameters. It also produces noise-free ground-truth labels.
                We introduce a novel hairstyle taxonomy developed in collaboration with a diverse group of domain experts which we use to balance our training data, supervise our model, and directly measure fairness.
                We annotate our synthetic training data and a real evaluation dataset using this taxonomy and release both to enable comparison of future hairstyle prediction approaches.
                We employ an architecture based on a pre-trained feature extraction network in order to improve generalization of our method to real data and predict taxonomy attributes as an auxiliary task to improve accuracy.
                Results show our method to be significantly more robust for challenging hairstyles than recent parametric approaches.
                Evaluation with taxonomy-based metrics also demonstrates the fairness of our method across diverse hairstyles.
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Hairstyle Taxonomy
            </h1>
            <div class="content has-text-justified-desktop">
                <img class="separator" src="img/figures/taxonomy.png">
                <p>Graphical overview of our proposed hair taxonomy consisting of global and regional attributes. Note that some values are not visualized for the 'Gathered' and 'Length' attributes.</p>
                <p>Our hairstyle taxonomy consists of 18 attributes. 
                    There are ten global attributes which are based on the whole hairstyle, for example the shape of the hairline or surface appearance of the hair.
                    The scalp is divided into eight regions and each region is annotated with eight local attributes, such as length and strand styling.
                    So in total each hairstyle has 74 taxonomic labels.
                    While we hope that the taxonomy presented is sufficiently fair, objective and complete, we recognize that it is likely impossible for it to be truly complete.
                    We therefore encourage future work to extend the taxonomy as required and publish any modifications.</p>
            </div>
        </div>
    </section>
    
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Hairstyle Classification
            </h1>
            <div class="content has-text-justified-desktop">
                <img src="img/figures/architecture.png">
                <p>Training and operating scheme of the proposed model. Given an input image, frozen backbone <b><i>B</i></b> 
                    produces intermediate features that are later used to produce intermediate representation via shared layer 
                    <b><i>FC<sub>L</sub></i></b>. Given the output of <b><i>FC<sub>L</sub></i></b>: <i>(i)</i> hairstyle prediction
                     head <b><i>FC<sub>s</sub></i></b> predicts the final hairstyle and <i>(ii)</i> hairstyle attribute prediction heads <b><i>FC<sub>1</sub></i></b>, 
                    ..., <b><i>FC<sub>A</sub></i></b> predict taxonomic hairstyle attributes.</p>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Qualitative Results
            </h1>
            <div class="content has-text-justified-desktop" style="align-items: center;">
                <img src="img/figures/qualitative_results.png">
                <div style="transform: translate(3%, 0%);">
                    <img src="img/figures/qualitative_results_header.png" width="90%">
                </div>
                <p>Comparison of our method with recent parametric hair prediction approach <cite><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_HairStep_Transfer_Synthetic_to_Real_Using_Strand_and_Depth_Maps_CVPR_2023_paper.html">HairStep</a></cite>.
    HairStep performs well for long straight hair (top row), but has a strong bias towards this hair style and type.
    This results in poor performance for short styles and coily or curly hair types, even if results appear to be of reasonable quality when viewed from the front. While our results provide less direct representation in some cases, they are significantly more robust across diverse hairstyles. 
    HairStep results are manually aligned to reconstructed face meshes. FairFace images -- CC BY 4.0.</p>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Failure Cases
            </h1>
            <div class="content has-text-justified-desktop">
                <img src="img/figures/failures.jpg">
            </div>
        </div>
    </section>

    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                BibTeX
            </h1>
            <pre>
@inproceedings{meishvili2024hairmony,
    title={Hairmony: Fairness-aware hairstyle classification},
    author={Meishvili, Givi and Clemoes, James and Hewitt, Charlie and Hosenie, Zafiirah and Xian, Xiao and de La Gorce, Martin and Takacs, Tibor and Baltru\v{s}aitis, Tadas and Criminisi, Antonio and McRae, Chyna and Jablonski, Nina and Wilczkowiak, Marta},
    booktitle={Proceedings of SIGGRAPH Asia},
    year={2024}
}</pre>
        </div>
    </section>
    <footer class="footer pb-0">
        <div class="content has-text-centered pb-5">
            <p>
                Work conducted at the <a href=https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge>Mixed Reality & AI Lab &ndash; Cambridge</a>.<br>
                <img src="img/Microsoft-logo-only.svg" class="mt-5" alt="Microsoft" style="height: 2rem;">
            </p>
        </div>
        <div class="footer-links content has-text-centered pt-5 has-text-grey-lighter is-size-7">
            <a href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy</a>
            <a href="https://go.microsoft.com/fwlink/?LinkID=206977">Terms of Use</a>
            <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks">Trademarks</a>
            <a href="https://microsoft.com">&copy; Microsoft 2024</a>
        </div>
    </footer>
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>
