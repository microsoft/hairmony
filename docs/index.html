<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Hairmony</title>
    <link rel="shortcut icon" type="image/jpg" href="img/favicon.ico" />
    <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css"> -->
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>

</head>

<body>
    <nav class="navbar is-dark" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item"
                    href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                    <img src="img/Microsoft-logo.svg" alt="Mixed Reality & AI Lab â€“ Cambridge" style="height: 1.4rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false"
                    data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-start">
                    <a class="navbar-item"
                        href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                        Mixed Reality & AI
                    </a>
                </div>
                <div class="navbar-end">
                    <a class="navbar-item" href="https://asia.siggraph.org/2024/">
                        <img class="is-hidden-touch" src="img/sa-logo-white.png" alt="SIGGRAPH Asia 2024">
                        <img class="is-hidden-desktop" src="img/sa-logo-black.png" alt="SIGGRAPH Asia 2024">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-2 has-text-centered">Hairmony</h1>
            <p class="subtitle is-4 has-text-centered">Fairness-aware hairstyle classification</p>
            <p class="subtitle is-5 has-text-centered has-text-grey">SIGGRAPH Asia 2024</p>
            <p class="subtitle is-6 has-text-centered authors" style="line-height: 1.5;">
                <span><a href="https://gmeishvili.github.io/" class="has-tooltip-bottom"
                        data-tooltip="* denotes equal contribution">Givi&nbsp;Meishvili</a><sup>*</sup></span>
                <span><a href="mailto:jaclemoe@microsoft.com" class="has-tooltip-bottom"
                        data-tooltip="* denotes equal contribution">James&nbsp;Clemoes</a><sup>*</sup></span>
                <span><a href="https://chewitt.me/" class="has-tooltip-bottom"
                        data-tooltip="* denotes equal contribution">Charlie&nbsp;Hewitt</a><sup>*</sup></span>
                <span><a href="mailto:zhosenie@microsoft.com">Zafiirah&nbsp;Hosenie</a></span>
                <span><a href="mailto:xianxiao@microsoft.com">Xiao-Xian</a></span>
                <span><a href="mailto:madelago@microsoft.com">Martin&nbsp;de&nbsp;La&nbsp;Gorce</a></span>
                <span><a href="mailto:Tibor.Takacs@microsoft.com">Tibor&nbsp;Takacs</a></span>
                <span><a href="mailto:tabaltru@microsoft.com">Tadas&nbsp;Baltru&scaron;aitis</a></span>
                <span><a href="mailto:acriminisi@microsoft.com">Antonio&nbsp;Criminisi</a></span>
                <span><a href="mailto:chynamcrae@microsoft.com">Chyna&nbsp;McRae</a></span>
                <span><a href="mailto:ngj2@psu.edu">Nina&nbsp;Jablonski</a></span>
                <span><a href="mailto:mawilczk@microsoft.com">Marta&nbsp;Wilczkowiak</a></span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <a class="button is-rounded is-link is-light mr-2" disabled>
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
            </a>
            <a href="TODO" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <a href="TODO" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fab fa-youtube" aria-hidden="true"></i></span>
                <span>Video</span>
            </a>
            <a href="https://github.com/microsoft/hairmony" class="button is-rounded is-link is-light">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Dataset</span>
            </a>
        </div>
    </section>
    <section>
        <div class="container is-max-desktop">
            <figure class="image is-16by9">
                <iframe class="has-ratio" width="640" height="360" src="TODO" frameborder="0" allowfullscreen></iframe>
            </figure>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                We present a method for prediction of a person's hairstyle from a single image.
                Despite growing use cases in user digitization and enrollment for virtual experiences, available methods
                are limited, particularly in the range of hairstyles they can capture.
                Human hair is extremely diverse and lacks any universally accepted description or categorization, making
                this a challenging task.
                Most current methods rely on parametric models of hair at a strand level.
                These approaches, while very promising, are not yet able to represent short, frizzy, coily hair and
                gathered hairstyles.
                We instead choose a classification approach which <i>can</i> represent the diversity of hairstyles
                required for a truly robust and inclusive system.
                Previous classification approaches have been restricted by poorly labeled data that lacks diversity,
                imposing constraints on the usefulness of any resulting enrollment system.
                We use only synthetic data to train our models. This allows for explicit control of diversity of
                hairstyle attributes, hair colors, facial appearance, poses, environments and other parameters. It also
                produces noise-free ground-truth labels.
                We introduce a novel hairstyle taxonomy developed in collaboration with a diverse group of domain
                experts which we use to balance our training data, supervise our model, and directly measure fairness.
                We annotate our synthetic training data and a real evaluation dataset using this taxonomy and release
                both to enable comparison of future hairstyle prediction approaches.
                We employ an architecture based on a pre-trained feature extraction network in order to improve
                generalization of our method to real data and predict taxonomy attributes as an auxiliary task to
                improve accuracy.
                Results show our method to be significantly more robust for challenging hairstyles than recent
                parametric approaches.
                Evaluation with taxonomy-based metrics also demonstrates the fairness of our method across diverse
                hairstyles.
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Hairstyle Taxonomy
            </h1>
            <div class="content has-text-justified-desktop">
                <p>Our hairstyle taxonomy consists of 18 attributes.
                    There are ten global attributes which are based on the whole hairstyle, for example the shape of the
                    hairline or surface appearance of the hair.
                    The scalp is divided into eight regions and each region is annotated with eight local attributes,
                    such as length and strand styling.
                    So in total each hairstyle has 74 taxonomic labels.
                    While we hope that the taxonomy presented is sufficiently fair, objective and complete, we recognize
                    that it is likely impossible for it to be truly complete.
                    We therefore encourage future work to extend the taxonomy as required and publish any modifications
                    - full details of the taxonomy can be found <a
                        href="https://github.com/microsoft/hairmony/tree/main/taxonomy">here</a>.
                </p>
                <figure class="ml-0 mr-0">
                    <img src="img/figures/taxonomy.png">
                    <figcaption>Graphical overview of our proposed hair taxonomy consisting of global and regional
                        attributes. Note
                        that some values are not visualized for the 'Gathered' and 'Length' attributes.</figcaption>
                </figure>
            </div>
        </div>
    </section>

    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Hairstyle Classification
            </h1>
            <div class="content has-text-justified-desktop">
                <p>We use a convolutional neural network (CNN) trained entirely on synthetic data.
                    The network is comprised of a frozen backbone that has been pre-trained through self-supervision 
                    on a large corpus of real images to provide general-purpose visual features, DINOv2 followed by a 
                    number of fully-connected layers optimized during training, visualized below. We find using a frozen, 
                    pre-trained backbone to be highly beneficial for the task of hairstyle prediction.
                    Given the relatively small library of synthetic hairstyles, and limited visual quality of synthetic 
                    images it is very easy for neural networks to over-fit when trained exclusively on our synthetic data. 
                    High frequency details are also important to determine details of hair type and style, but the domain 
                    gap between real and synthetic images may hamper the ability of CNNs to learn these details when trained 
                    only on synthetic data. By using a model pre-trained on real images we minimize the ability of the network to over-fit, 
                    as we only optimize small fully connected layers, and ensure that we are extracting features that generalize to real images.
                    The primary task of our network is hairstyle prediction, formulated as classification task for hairstyles from our synthetic library.
                    We also include an auxiliary task of hairstyle attribute prediction, outputting the taxonomic annotations associated with the hairstyle.
                    Attributes are predicted by dedicated fully-connected heads which take shared features from a common fully-connected layer as input.
                    This architecture ensures that features from this intermediate layer are informed by attributes that we know are important 
                    (hair type, length, etc.) as determined by the taxonomy, rather than features indirectly inferred from a complex classification task.
                    The aim of this approach is to prevent 'bad' errors; that is to ensure that even if our method does not predict the perfect style, 
                    it at least predicts a style with matching attributes. A classification-only approach has no concept of this semantic similarity.</p>
                <figure class="ml-0 mr-0">
                    <img src="img/figures/architecture.png">
                    <figcaption>Training and operating scheme of the proposed model. Given an input image we use a frozen
                        backbone and independent MLP heads to predict hairstyle and taxonomic attributes.</figcaption>
                    </figcaption>
                </figure>
            </div>
        </div>
    </section>

    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Qualitative Results
            </h1>
            <div class="content has-text-justified-desktop" style="align-items: center;">
                <p>
                    We conducted a number of experiments assessing the quality of our method.
                    As we cannot compare to recent parametric approaches quantitatively without manual labeling of 
                    generated results with our taxonomy, we provide a qualitative comparison of our method with the 
                    state-of-the-art method for hairstyle reconstruction, shown in the figure below. 
                    While our method does not enable direct strand-wise representation, it is far more robust for diverse input hairstyles. 
                    Existing methods show a strong bias to straight, long hair while our method is able to provide appropriate hairstyle 
                    predictions for short, frizzy, coily and gathered styles, as well as long, straight hair.
                </p>
                <p>
                    HairStep performs well for long straight hair, but has a strong bias towards this hairstyle and type.
                    This results in poor performance for short styles and coily or curly hair types, even if results
                    appear to be of reasonable quality when viewed from the front. While our results provide less
                    direct representation in some cases, they are significantly more robust across diverse hairstyles.
                </p>
                <figure class="ml-0 mr-0">
                    <div class="columns m-0 mb-2 has-text-centered is-mobile is-size-7-touch">
                        <div class="column p-0">Input</div>
                        <div class="column p-0">HairStep<br>(strands)</div>
                        <div class="column p-0">HairStep<br>(front)</div>
                        <div class="column p-0">HairStep<br>(side)</div>
                        <div class="column p-0"><b>Ours<br>(front)</b></div>
                        <div class="column p-0"><b>Ours<br>(side)</b></div>
                    </div>
                    <img src="img/figures/qualitative_results.png">
                    <figcaption>Comparison of our method with recent parametric hair prediction approach <cite><a
                                href="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_HairStep_Transfer_Synthetic_to_Real_Using_Strand_and_Depth_Maps_CVPR_2023_paper.html">HairStep</a></cite>.
                        HairStep results are manually aligned to reconstructed face meshes.</figcaption>
                </figure>
                <figure class="ml-0 mr-0">
                    <img src="img/figures/more_results.jpg">
                    <img src="img/figures/failures.jpg">
                    <figcaption>Qualitative results for our method on the <cite><a
                                href="https://openaccess.thecvf.com/content/WACV2021/papers/Karkkainen_FairFace_Face_Attribute_Dataset_for_Balanced_Race_Gender_and_Age_WACV_2021_paper.pdf">FairFace</a></cite>
                        evaluation subset.
                        Bottom row shows failure cases, specifically: missed hair, incorrect hair type/strand styling,
                        incorrect gathering, incorrect length, hairstyle not in library.</figcaption>
                </figure>
            </div>
        </div>
    </section>

    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                BibTeX
            </h1>
            <pre>
@inproceedings{meishvili2024hairmony,
    title={Hairmony: Fairness-aware hairstyle classification},
    author={Meishvili, Givi and Clemoes, James and Hewitt, Charlie and Hosenie, Zafiirah and Xian, Xiao and de La Gorce, Martin and Takacs, Tibor and Baltru\v{s}aitis, Tadas and Criminisi, Antonio and McRae, Chyna and Jablonski, Nina and Wilczkowiak, Marta},
    booktitle={SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers '24), December 3-6, Tokyo, Japan},
    year={2024},
}</pre>
        </div>
    </section>
    <footer class="footer pb-0">
        <div class="content has-text-centered pb-5">
            <p>
                Work conducted at the <a
                    href=https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge>Mixed Reality & AI
                    Lab &ndash; Cambridge</a>.<br>
                <img src="img/Microsoft-logo-only.svg" class="mt-5" alt="Microsoft" style="height: 2rem;">
            </p>
        </div>
        <div class="footer-links content has-text-centered pt-5 has-text-grey-lighter is-size-7">
            <a href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy</a>
            <a href="https://go.microsoft.com/fwlink/?LinkID=206977">Terms of Use</a>
            <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks">Trademarks</a>
            <a href="https://microsoft.com">&copy; Microsoft 2024</a>
        </div>
    </footer>
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>